
# ðŸ‘¾ Designing Conversations: Improving Human Interactions with AI
Welcome to the "Designing conversations: improving human interactions with AI" repository! This repository contains example resources for improving human interactions with AI through designing effective prompt.

## Overview
AI systems are becoming increasingly integrated into our daily lives, from chatbots that help us with customer service inquiries to voice assistants that help us with tasks around the house, from Dall-2 that help us with generating pictures to ChatGPT that assist us with responsing human-like text. By providing it with a prompt, it can generate responses that continue the conversation or expand on the given prompt. As these systems become more prevalent, it is essential that they are designed in a way that facilitates effective and meaningful conversations or prompts between humans and machines.

This repository provides resources and guidance for designing conversations that improve human interactions with AI. The resources in this repository include:

- Articles and research papers on conversation design and AI
- Code examples and tutorials for building conversational interfaces
- Best practices for designing effective prompts with AI
- Public resources for AI models and prompts
- Case studies and examples of successful conversational AI implementations

## Getting Started
To get started with designing effective conversations with AI, you can explore the resources in this repository. Start by reading through the articles and research papers to gain a deeper understanding of conversation design principles and best practices.

You can also explore the code examples and tutorials to get hands-on experience with building conversational interfaces. The tools and frameworks provided in this repository can help you streamline your design process and test your conversational AI systems.

## Contributing
Contributions to this repository are welcome! If you have resources or tools that you would like to add, please submit a pull request. If you find an issue or would like to request a feature, please open an issue in the repository.

## Models and Examples:

- [ðŸ‘ChatGPT](./ChatGPT/readme.md)
> ðŸ‘ ChatGPT
>
>  A large language model developed by OpenAI, based on the GPT-3.5 architecture. It has a diverse range of capabilities, including natural language processing, language translation, text summarization, sentiment analysis, and more. It can be used for a wide range of tasks, from answering trivia questions and providing personalized recommendations to conducting research and generating creative writing  [[1]](#1)


- [ðŸ“˜DALL-E2](./DALL-E2/readme.md)

> ðŸ“˜ DALL-E 2 
>
> an advanced version of DALL-E. It uses a combination of deep learning algorithms, including Transformers and GANs, to generate images that match the given text description.
The potential applications of DALL-E 2 are vast, including creating realistic product images for e-commerce websites, generating visual aids for scientific research, and even creating art and design elements for various projects.[[2]](#2)

- [ðŸŽ… Stable Duffusion](./Stable%20Duffusion/readme.md)

> ðŸŽ… Stable Diffusion
>
> a probabilistic generative model for image and video generation, developed by researchers at Google AI. 
The Stable Diffusion Model is particularly well-suited for generating large images or long videos, and is capable of capturing complex dependencies between the different elements in the image or video. It has been shown to outperform other state-of-the-art generative models in terms of visual quality and diversity.[[3]](#3)

- [ðŸ¥‡ PaLM-E](./PaLM-E/readme.md)

> ðŸ¥‡ PaLM-E
>
> PaLM-E (Pre-training and Language Model for English) is a large-scale language model developed by the research team at Facebook AI. 
The model has potential applications in a variety of natural language processing tasks, including information retrieval, sentiment analysis, and machine translation. [[4]](#4)

- [ðŸ”– GPT4]

> ðŸ”– GPT4
>
> a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks. [[5]](#5)

## License
This repository is licensed under the MIT license. Please see the LICENSE file for more information.

## Acknowledgments
This repository was inspired by the work of many researchers and practitioners in the field of conversation design and AI. We would like to thank them for their contributions and inspiration.


We hope you find these prompts useful and have fun using AI models!

# Some Public Promots Resources

- [awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)

- [ChatGPT ä¸­æ–‡è°ƒæ•™æŒ‡å—](https://github.com/PlexPt/awesome-chatgpt-prompts-zh)

- [DALL-E-2-prompt-guide](https://strikingloo.github.io/DALL-E-2-prompt-guide)

- [DALLÂ·E Editor Guide](https://help.openai.com/en/articles/6516417-dall-e-editor-guide)

- [5 Tips to Get the Best Results From DALL-E 2](https://www.howtogeek.com/836690/5-tips-to-get-the-best-results-from-dall-e-2/)

- [Reverse Engineered ChatGPT API](https://github.com/acheong08/ChatGPT)

# Cases
- [https://byteclicks.com/30078.html](https://byteclicks.com/30078.html)
- [ChatGPTä¼šå–ä»£æœç´¢å¼•æ“Žå—](https://zhuanlan.zhihu.com/p/589533490)
- [æ€Žæ ·è®©ChatGPTåœ¨å…¶å†…éƒ¨è®­ç»ƒç¥žç»ç½‘ç»œ](https://zhuanlan.zhihu.com/p/605163615)
- [AI-powered Bing Chat spills its secrets via prompt injection attack](https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/)

# Company
- [jasper](https://www.jasper.ai/)

# Models
| Model Name  | Parameters | Open |
| ------------- | ------------- |------------- |
| BERT  | 340 million | yes |
| ELMo-BiDAF | 113 million | - |
| [Turing NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)  | 17 billion  | - |
| [Chinchilla LLM](https://towardsdatascience.com/a-new-ai-trend-chinchilla-70b-greatly-outperforms-gpt-3-175b-and-gopher-280b-408b9b4510)  | 70 billion  | - |
| Gopher | 280 billion | - |
| Brain  | 2.5 trillion  | - |
| [GPT-2](https://github.com/openai/gpt-2)  | 1.5 billion  | yes |
| GPT-3.5  | 175 billion  | No |
| GPT-4  | >175 billion  | - |
| Jurassic-1 | 178 billion | - |
| LaMDA | 137 billion | - |
| Megatron-Turing NLG | 530 billion | - |
| Wudao  | 1.75 Trillion | - |
| UNITER-base   | 86 million | - |
| UNITER-large  | 303 million | - |
| FastMoE| trillions | yes |
| HuggingFace|1.6 trillion| - |

# Benchmark achievements
| Benchmark | Achievements |
| ------------- | ------------- |
|ImageNet (zero-shot)| SOTA, surpassing OpenAI CLIP.|
|LAMA (factual and commonsense knowledge)| Surpassed AutoPrompt.|
|LAMBADA (cloze tasks)| Surpassed Microsoft Turing NLG.|
|SuperGLUE (few-shot)| SOTA, surpassing OpenAI GPT-3.|
|UC Merced Land Use (zero-shot)| SOTA, surpassing OpenAI CLIP.|
|MS COCO (text generation diagram)| Surpassed OpenAI DALLÂ·E.|
|MS COCO (English graphic retrieval)| Surpassed OpenAI CLIP and Google ALIGN.|
|MS COCO (multilingual graphic retrieval)| Surpassed UCÂ² (best multilingual and multimodal pre-trained model).|
|Multi 30K (multilingual graphic retrieval)| Surpassed UCÂ².|


## References
<a id="1">[1]</a> Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems (pp. 1877-1901).

<a id="2">[2]</a>Ramesh, A., Goyal, A., Raghu, M., Sohl-Dickstein, J., & Bengio, S. (2021). DALL-E 2: The Effective Use of Transformer on Image Generation. arXiv preprint arXiv:2108.05355.

<a id="3">[3]</a> Ho, J., Chen, X., Srinivas, A., Duan, Y., Abbeel, P., & Finn, C. (2020). Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (pp. 3393-3403).

<a id="4">[4]</a>Khandelwal, U., He, Z., Qi, P., & Jurafsky, D. (2021). PaLM: Pluggable and Latent Modeling for Large-scale Natural Language Understanding. arXiv preprint arXiv:2103.13791.

<a id="5">[5]</a> OpenAI (2023). GPT-4 Technical Report. link: https://cdn.openai.com/papers/gpt-4.pdf





